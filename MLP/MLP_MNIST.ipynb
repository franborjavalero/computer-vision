{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de MLP_MNIST.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Zn9BACY199lU","colab_type":"code","outputId":"4a0e9b74-54cf-4b82-94c7-aaca1eddacc3","executionInfo":{"status":"ok","timestamp":1578835063082,"user_tz":-60,"elapsed":20299,"user":{"displayName":"Fran Borja","photoUrl":"","userId":"06434405742332867590"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["https://bit.ly/2TTGDw8from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CRtKohum-AQw","colab_type":"text"},"source":["# **Preprocessing**"]},{"cell_type":"code","metadata":{"id":"QxgSLoLR9_ME","colab_type":"code","colab":{}},"source":[" # reproducibility\n"," seed = 42\n","import numpy as np\n","np.random.seed(seed)\n","import torch\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","import torchvision\n","from torchvision import transforms, datasets\n","download_ = False\n","device = torch.device(\"cuda:0\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtT41O4q-Ufa","colab_type":"code","colab":{}},"source":["path_data = \"/content/drive/My Drive/Colab Notebooks/RNA/Practica2/data/\"\n","!mkdir -p \"/content/drive/My Drive/Colab Notebooks/RNA/Practica2/data/\"\n","# Dataset is not stored like a tensor, for this reasones is made the transformation, pixel/255, normalized [0,1]\n","# variables train, test; are type dataset, located in a real directory\n","\n","# DATA AUGMENTATION:\n","train_transforms = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(p=0.0),                                          \n","                                                   transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n","                                                   torchvision.transforms.ToTensor()])\n","transforms_ = transforms.Compose([transforms.ToTensor()])\n","train = datasets.MNIST(path_data, train=True, download=download_, transform=train_transforms)\n","test = datasets.MNIST(path_data, train=False, download=download_, transform=transforms_ )\n","path_mnist = f\"{path_data}/MNIST\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzziVOmFBOqH","colab_type":"code","colab":{}},"source":["from math import ceil\n","batch_size = 64\n","num_traning_samples = len(train)\n","num_batches = ceil(num_traning_samples / batch_size)\n","num_test_samples = len(test)\n","# DataLoader is like a generator in python\n","train_set = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n","test_set = torch.utils.data.DataLoader(test, batch_size=num_test_samples, shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-EsGPQJHCed","colab_type":"text"},"source":["# **MLP for classification**"]},{"cell_type":"code","metadata":{"id":"-CT5d48nHG3p","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7U7Q6bwZx8v","colab_type":"code","colab":{}},"source":["# https://discuss.pytorch.org/t/writing-a-simple-gaussian-noise-layer-in-pytorch/4694/4\n","class GaussianNoise(nn.Module):\n","    def __init__(self, sigma, device=\"cpu\"):\n","        super().__init__()\n","        self.sigma = sigma\n","        self.device = device\n","\n","    def forward(self, x):\n","        if self.training and self.sigma != 0:\n","           # detach: es nueva view del tensor x, donde su contribucion (sampled noise) no se tiene en cuenta a la hora de calcular el gradiente.\n","           # http://www.bnikolic.co.uk/blog/pytorch-detach.html\n","            scale = self.sigma * x.detach()\n","            sampled_noise = torch.zeros(*x.size()).normal_().to(self.device).float() * scale\n","            x = x + sampled_noise\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UB69q9iwI3o8","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","  \n","  def __init__(self):\n","    super().__init__()\n","    dim_ = 1024\n","    # input images 28*28 -> flatten matrix to a vector 784\n","    self.fc1 = nn.Linear(28*28, dim_) # dim_ units, linear is a fully connected layer\n","    self.bn1 = nn.BatchNorm1d(dim_)\n","    self.gn1 = GaussianNoise(0.3, device=device)\n","    self.fc2 = nn.Linear(dim_, dim_) # dim_ units\n","    self.bn2 = nn.BatchNorm1d(dim_)\n","    self.gn2 = GaussianNoise(0.3, device=device)\n","    self.fc3 = nn.Linear(dim_, dim_) # dim_ units\n","    self.bn3 = nn.BatchNorm1d(dim_)\n","    self.gn3 = GaussianNoise(0.3, device=device)\n","    self.fc4 = nn.Linear(dim_, 10) # dim_ units\n","  \n","  def forward(self, x):\n","    s1 = F.relu(self.gn1(self.bn1(self.fc1(x))))\n","    s2 = F.relu(self.gn2(self.bn2(self.fc2(s1))))\n","    s3 = F.relu(self.gn3(self.bn3(self.fc3(s2))))    \n","    logits = self.fc4(s3)\n","    return logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GOApyN1uKml4","colab_type":"code","outputId":"c4cba75d-768b-4538-b0ee-307c923617d8","executionInfo":{"status":"ok","timestamp":1578835180793,"user_tz":-60,"elapsed":10119,"user":{"displayName":"Fran Borja","photoUrl":"","userId":"06434405742332867590"}},"colab":{"base_uri":"https://localhost:8080/","height":251}},"source":["# torch.cuda.Tensor occupies GPU memory. Of course operations on a GPU / CUDA Tensor are computed on GPU.\n","net = Net().to(device) # torch cuda tensors, instead normal torch tensors (cpu)\n","print(net)\n","print(net.fc1.weight.type())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Net(\n","  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n","  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (gn1): GaussianNoise()\n","  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (gn2): GaussianNoise()\n","  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n","  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (gn3): GaussianNoise()\n","  (fc4): Linear(in_features=1024, out_features=10, bias=True)\n",")\n","torch.cuda.FloatTensor\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FUsBa0MZO26e","colab_type":"text"},"source":["# **Training**"]},{"cell_type":"code","metadata":{"id":"cmHiamKVO7Q1","colab_type":"code","colab":{}},"source":["import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"02MbdCCuPBtk","colab_type":"code","colab":{}},"source":["model_name = \"mlp_bn_gn_lra_da\"\n","lr_=0.1\n","num_epochs =  100\n","#patience = 15\n","optimizer = optim.SGD(net.parameters(), lr=lr_, weight_decay=1e-6, momentum=0.9)\n","#optimizer = optim.SGD(net.parameters(), lr=lr_)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=lr_)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkltI4-mQOyD","colab_type":"code","outputId":"46a42b03-895d-47bb-935a-ac5929ffb767","executionInfo":{"status":"ok","timestamp":1578836565588,"user_tz":-60,"elapsed":1376198,"user":{"displayName":"Fran Borja","photoUrl":"","userId":"06434405742332867590"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","acc_epoch_train = np.full((num_epochs), np.inf)\n","acc_epoch_test = np.full((num_epochs), np.inf)\n","cost_epoch_train = np.full((num_epochs), np.inf)\n","cost_epoch_test = np.full((num_epochs), np.inf)\n","\n","best_cost_test = np.inf\n","\n","dir_checkpoint_model = \"/content/drive/My Drive/Colab Notebooks/RNA/Practica2/model_checkpoints/MNIST/\"\n","if not os.path.exists(dir_checkpoint_model):\n","    os.makedirs(dir_checkpoint_model)\n","path_checkpoint_model = f\"{dir_checkpoint_model}{model_name}.pth\"\n","confussion_matrix_ = None\n","\n","# restore checkpoint \n","# checkpoint_ = torch.load(path_checkpoint_model)\n","# net.load_state_dict(checkpoint_['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint_['optimizer_state_dict'])\n","# best_cost_test = checkpoint_[\"cost\"]\n","            \n","#epochs_patience = 0\n","for id_epoch in range(0, num_epochs, 1):\n","  # TRAIN\n","  num_corrected_predictions_train = 0\n","  total_loss_train = 0\n","  for idx_batch, (batch_x, batch_y) in enumerate(train_set):\n","    batch_x_ = batch_x.view(-1, 28*28).to(device)\n","    batch_y_ = batch_y.to(device)\n","    optimizer.zero_grad() # initiated to zero the gradients, very usefull rnn for acummulating\n","    predictions = net(batch_x_) # -1 means all de examples of the batch, in each one, beacuse the last one can have different number of examples\n","    #loss = F.nll_loss(predictions, batch_y_) # F.nll_loos, the negative log likelihood loss, before was applied the softmax layer: F.log_softmax\n","    loss = criterion(predictions, batch_y_)\n","    loss.backward()\n","    optimizer.step()\n","    _, predicted_labels = predictions.max(1) # get per sample the maximum value\n","    num_corrected_predictions_train += batch_y_.eq(predicted_labels).sum().item()\n","    total_loss_train += loss\n","  acc_train = (num_corrected_predictions_train / num_traning_samples)\n","  cost_train = (total_loss_train / num_batches)\n","  acc_epoch_train[id_epoch] = acc_train\n","  cost_epoch_train[id_epoch] = cost_train\n","  # EVAL\n","  with torch.no_grad():\n","    for batch_x_test, batch_y_test in test_set:\n","      batch_x_ = batch_x_test.view(-1, 28*28).to(device)\n","      batch_y_ = batch_y_test.to(device)\n","      predictions = net(batch_x_)\n","      _, predicted_labels = predictions.max(1) # get per sample the maximum value\n","      acc_test = batch_y_.eq(predicted_labels).sum().item()/num_test_samples\n","      #cost_test = torch.mean(F.nll_loss(predictions, batch_y_))\n","      cost_test = criterion(predictions, batch_y_)\n","      acc_epoch_test[id_epoch] = acc_test\n","      cost_epoch_test[id_epoch] = cost_test\n","      if cost_test < best_cost_test:\n","        predictions_labels_ = predicted_labels.to(\"cpu\")\n","        confusion_matrix_ = confusion_matrix(batch_y_test, predictions_labels_)\n","        best_cost_test = cost_test\n","        #epochs_patience = 0\n","        # save model\n","        # torch.save({\n","        #     'epoch': id_epoch+1,\n","        #     'model_state_dict': net.state_dict(),\n","        #     'optimizer_state_dict': optimizer.state_dict(),\n","        #     'costt': cost_test,\n","        # }, path_checkpoint_model)\n","        confusion_matrix(batch_y_test, predictions_labels_)\n","  #     else:\n","  #       epochs_patience += 1\n","  # if epochs_patience >= patience:\n","  #   break\n","  \n","  scheduler.step()\n","\n","  print(f\"Epoch {id_epoch}:\\n \\tcost train: {cost_train:.3f}\\n \\tcost test: {cost_test:.3f}\\n \\tacc train: {acc_train*100:.3f}\\n \\tacc test: {acc_test*100:.3f}\\n\")\n","\n","acc_epoch_train = acc_epoch_train[:id_epoch]\n","acc_epoch_test = acc_epoch_test[:id_epoch]\n","cost_epoch_train = cost_epoch_train[:id_epoch]\n","cost_epoch_test = cost_epoch_test[:id_epoch]\n","\n","print(f\"Best test-acc: {np.max(acc_epoch_test):.4f}\")    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0:\n"," \tcost train: 0.560\n"," \tcost test: 0.155\n"," \tacc train: 85.232\n"," \tacc test: 95.170\n","\n","Epoch 1:\n"," \tcost train: 0.220\n"," \tcost test: 0.117\n"," \tacc train: 93.265\n"," \tacc test: 96.340\n","\n","Epoch 2:\n"," \tcost train: 0.169\n"," \tcost test: 0.094\n"," \tacc train: 94.752\n"," \tacc test: 96.900\n","\n","Epoch 3:\n"," \tcost train: 0.149\n"," \tcost test: 0.069\n"," \tacc train: 95.298\n"," \tacc test: 97.710\n","\n","Epoch 4:\n"," \tcost train: 0.136\n"," \tcost test: 0.070\n"," \tacc train: 95.622\n"," \tacc test: 97.630\n","\n","Epoch 5:\n"," \tcost train: 0.119\n"," \tcost test: 0.067\n"," \tacc train: 96.352\n"," \tacc test: 97.910\n","\n","Epoch 6:\n"," \tcost train: 0.112\n"," \tcost test: 0.073\n"," \tacc train: 96.438\n"," \tacc test: 97.830\n","\n","Epoch 7:\n"," \tcost train: 0.108\n"," \tcost test: 0.063\n"," \tacc train: 96.600\n"," \tacc test: 98.010\n","\n","Epoch 8:\n"," \tcost train: 0.102\n"," \tcost test: 0.054\n"," \tacc train: 96.795\n"," \tacc test: 98.180\n","\n","Epoch 9:\n"," \tcost train: 0.098\n"," \tcost test: 0.063\n"," \tacc train: 96.943\n"," \tacc test: 97.940\n","\n","Epoch 10:\n"," \tcost train: 0.090\n"," \tcost test: 0.064\n"," \tacc train: 97.122\n"," \tacc test: 97.910\n","\n","Epoch 11:\n"," \tcost train: 0.091\n"," \tcost test: 0.048\n"," \tacc train: 97.115\n"," \tacc test: 98.330\n","\n","Epoch 12:\n"," \tcost train: 0.086\n"," \tcost test: 0.052\n"," \tacc train: 97.330\n"," \tacc test: 98.420\n","\n","Epoch 13:\n"," \tcost train: 0.082\n"," \tcost test: 0.051\n"," \tacc train: 97.383\n"," \tacc test: 98.400\n","\n","Epoch 14:\n"," \tcost train: 0.080\n"," \tcost test: 0.053\n"," \tacc train: 97.445\n"," \tacc test: 98.330\n","\n","Epoch 15:\n"," \tcost train: 0.077\n"," \tcost test: 0.049\n"," \tacc train: 97.550\n"," \tacc test: 98.410\n","\n","Epoch 16:\n"," \tcost train: 0.074\n"," \tcost test: 0.049\n"," \tacc train: 97.582\n"," \tacc test: 98.500\n","\n","Epoch 17:\n"," \tcost train: 0.071\n"," \tcost test: 0.043\n"," \tacc train: 97.737\n"," \tacc test: 98.630\n","\n","Epoch 18:\n"," \tcost train: 0.073\n"," \tcost test: 0.047\n"," \tacc train: 97.703\n"," \tacc test: 98.570\n","\n","Epoch 19:\n"," \tcost train: 0.068\n"," \tcost test: 0.042\n"," \tacc train: 97.817\n"," \tacc test: 98.640\n","\n","Epoch 20:\n"," \tcost train: 0.069\n"," \tcost test: 0.043\n"," \tacc train: 97.815\n"," \tacc test: 98.600\n","\n","Epoch 21:\n"," \tcost train: 0.066\n"," \tcost test: 0.043\n"," \tacc train: 97.965\n"," \tacc test: 98.570\n","\n","Epoch 22:\n"," \tcost train: 0.062\n"," \tcost test: 0.039\n"," \tacc train: 98.000\n"," \tacc test: 98.750\n","\n","Epoch 23:\n"," \tcost train: 0.062\n"," \tcost test: 0.036\n"," \tacc train: 98.015\n"," \tacc test: 98.820\n","\n","Epoch 24:\n"," \tcost train: 0.064\n"," \tcost test: 0.039\n"," \tacc train: 97.972\n"," \tacc test: 98.670\n","\n","Epoch 25:\n"," \tcost train: 0.061\n"," \tcost test: 0.043\n"," \tacc train: 98.043\n"," \tacc test: 98.590\n","\n","Epoch 26:\n"," \tcost train: 0.061\n"," \tcost test: 0.042\n"," \tacc train: 98.097\n"," \tacc test: 98.600\n","\n","Epoch 27:\n"," \tcost train: 0.060\n"," \tcost test: 0.044\n"," \tacc train: 98.077\n"," \tacc test: 98.530\n","\n","Epoch 28:\n"," \tcost train: 0.058\n"," \tcost test: 0.044\n"," \tacc train: 98.097\n"," \tacc test: 98.540\n","\n","Epoch 29:\n"," \tcost train: 0.059\n"," \tcost test: 0.038\n"," \tacc train: 98.117\n"," \tacc test: 98.720\n","\n","Epoch 30:\n"," \tcost train: 0.057\n"," \tcost test: 0.038\n"," \tacc train: 98.193\n"," \tacc test: 98.740\n","\n","Epoch 31:\n"," \tcost train: 0.056\n"," \tcost test: 0.035\n"," \tacc train: 98.197\n"," \tacc test: 98.840\n","\n","Epoch 32:\n"," \tcost train: 0.056\n"," \tcost test: 0.033\n"," \tacc train: 98.230\n"," \tacc test: 98.980\n","\n","Epoch 33:\n"," \tcost train: 0.055\n"," \tcost test: 0.032\n"," \tacc train: 98.288\n"," \tacc test: 98.910\n","\n","Epoch 34:\n"," \tcost train: 0.053\n"," \tcost test: 0.042\n"," \tacc train: 98.335\n"," \tacc test: 98.670\n","\n","Epoch 35:\n"," \tcost train: 0.053\n"," \tcost test: 0.038\n"," \tacc train: 98.387\n"," \tacc test: 98.720\n","\n","Epoch 36:\n"," \tcost train: 0.051\n"," \tcost test: 0.039\n"," \tacc train: 98.338\n"," \tacc test: 98.780\n","\n","Epoch 37:\n"," \tcost train: 0.051\n"," \tcost test: 0.044\n"," \tacc train: 98.345\n"," \tacc test: 98.690\n","\n","Epoch 38:\n"," \tcost train: 0.052\n"," \tcost test: 0.039\n"," \tacc train: 98.330\n"," \tacc test: 98.780\n","\n","Epoch 39:\n"," \tcost train: 0.051\n"," \tcost test: 0.041\n"," \tacc train: 98.333\n"," \tacc test: 98.590\n","\n","Epoch 40:\n"," \tcost train: 0.049\n"," \tcost test: 0.033\n"," \tacc train: 98.378\n"," \tacc test: 98.940\n","\n","Epoch 41:\n"," \tcost train: 0.050\n"," \tcost test: 0.038\n"," \tacc train: 98.383\n"," \tacc test: 98.820\n","\n","Epoch 42:\n"," \tcost train: 0.049\n"," \tcost test: 0.032\n"," \tacc train: 98.442\n"," \tacc test: 98.890\n","\n","Epoch 43:\n"," \tcost train: 0.047\n"," \tcost test: 0.040\n"," \tacc train: 98.458\n"," \tacc test: 98.750\n","\n","Epoch 44:\n"," \tcost train: 0.047\n"," \tcost test: 0.035\n"," \tacc train: 98.532\n"," \tacc test: 98.850\n","\n","Epoch 45:\n"," \tcost train: 0.046\n"," \tcost test: 0.038\n"," \tacc train: 98.500\n"," \tacc test: 98.680\n","\n","Epoch 46:\n"," \tcost train: 0.046\n"," \tcost test: 0.042\n"," \tacc train: 98.510\n"," \tacc test: 98.530\n","\n","Epoch 47:\n"," \tcost train: 0.047\n"," \tcost test: 0.039\n"," \tacc train: 98.495\n"," \tacc test: 98.740\n","\n","Epoch 48:\n"," \tcost train: 0.046\n"," \tcost test: 0.039\n"," \tacc train: 98.525\n"," \tacc test: 98.710\n","\n","Epoch 49:\n"," \tcost train: 0.044\n"," \tcost test: 0.039\n"," \tacc train: 98.603\n"," \tacc test: 98.810\n","\n","Epoch 50:\n"," \tcost train: 0.038\n"," \tcost test: 0.030\n"," \tacc train: 98.818\n"," \tacc test: 99.030\n","\n","Epoch 51:\n"," \tcost train: 0.034\n"," \tcost test: 0.030\n"," \tacc train: 98.950\n"," \tacc test: 98.920\n","\n","Epoch 52:\n"," \tcost train: 0.033\n"," \tcost test: 0.028\n"," \tacc train: 98.888\n"," \tacc test: 99.100\n","\n","Epoch 53:\n"," \tcost train: 0.031\n"," \tcost test: 0.026\n"," \tacc train: 99.017\n"," \tacc test: 99.100\n","\n","Epoch 54:\n"," \tcost train: 0.030\n"," \tcost test: 0.028\n"," \tacc train: 98.990\n"," \tacc test: 99.120\n","\n","Epoch 55:\n"," \tcost train: 0.031\n"," \tcost test: 0.027\n"," \tacc train: 99.035\n"," \tacc test: 99.080\n","\n","Epoch 56:\n"," \tcost train: 0.029\n"," \tcost test: 0.028\n"," \tacc train: 98.987\n"," \tacc test: 99.130\n","\n","Epoch 57:\n"," \tcost train: 0.029\n"," \tcost test: 0.030\n"," \tacc train: 99.063\n"," \tacc test: 99.000\n","\n","Epoch 58:\n"," \tcost train: 0.029\n"," \tcost test: 0.027\n"," \tacc train: 98.995\n"," \tacc test: 99.000\n","\n","Epoch 59:\n"," \tcost train: 0.029\n"," \tcost test: 0.028\n"," \tacc train: 99.093\n"," \tacc test: 99.210\n","\n","Epoch 60:\n"," \tcost train: 0.028\n"," \tcost test: 0.027\n"," \tacc train: 99.083\n"," \tacc test: 99.100\n","\n","Epoch 61:\n"," \tcost train: 0.029\n"," \tcost test: 0.026\n"," \tacc train: 99.027\n"," \tacc test: 99.130\n","\n","Epoch 62:\n"," \tcost train: 0.028\n"," \tcost test: 0.026\n"," \tacc train: 99.112\n"," \tacc test: 99.140\n","\n","Epoch 63:\n"," \tcost train: 0.027\n"," \tcost test: 0.027\n"," \tacc train: 99.080\n"," \tacc test: 99.000\n","\n","Epoch 64:\n"," \tcost train: 0.028\n"," \tcost test: 0.027\n"," \tacc train: 99.113\n"," \tacc test: 99.100\n","\n","Epoch 65:\n"," \tcost train: 0.028\n"," \tcost test: 0.028\n"," \tacc train: 99.060\n"," \tacc test: 99.060\n","\n","Epoch 66:\n"," \tcost train: 0.027\n"," \tcost test: 0.024\n"," \tacc train: 99.133\n"," \tacc test: 99.220\n","\n","Epoch 67:\n"," \tcost train: 0.027\n"," \tcost test: 0.027\n"," \tacc train: 99.110\n"," \tacc test: 99.160\n","\n","Epoch 68:\n"," \tcost train: 0.027\n"," \tcost test: 0.027\n"," \tacc train: 99.162\n"," \tacc test: 99.150\n","\n","Epoch 69:\n"," \tcost train: 0.027\n"," \tcost test: 0.025\n"," \tacc train: 99.092\n"," \tacc test: 99.150\n","\n","Epoch 70:\n"," \tcost train: 0.025\n"," \tcost test: 0.025\n"," \tacc train: 99.163\n"," \tacc test: 99.160\n","\n","Epoch 71:\n"," \tcost train: 0.026\n"," \tcost test: 0.026\n"," \tacc train: 99.092\n"," \tacc test: 99.140\n","\n","Epoch 72:\n"," \tcost train: 0.027\n"," \tcost test: 0.027\n"," \tacc train: 99.138\n"," \tacc test: 99.100\n","\n","Epoch 73:\n"," \tcost train: 0.025\n"," \tcost test: 0.028\n"," \tacc train: 99.158\n"," \tacc test: 99.040\n","\n","Epoch 74:\n"," \tcost train: 0.026\n"," \tcost test: 0.027\n"," \tacc train: 99.145\n"," \tacc test: 99.040\n","\n","Epoch 75:\n"," \tcost train: 0.026\n"," \tcost test: 0.028\n"," \tacc train: 99.155\n"," \tacc test: 99.080\n","\n","Epoch 76:\n"," \tcost train: 0.025\n"," \tcost test: 0.027\n"," \tacc train: 99.168\n"," \tacc test: 99.140\n","\n","Epoch 77:\n"," \tcost train: 0.026\n"," \tcost test: 0.026\n"," \tacc train: 99.178\n"," \tacc test: 99.190\n","\n","Epoch 78:\n"," \tcost train: 0.026\n"," \tcost test: 0.027\n"," \tacc train: 99.135\n"," \tacc test: 99.160\n","\n","Epoch 79:\n"," \tcost train: 0.025\n"," \tcost test: 0.026\n"," \tacc train: 99.177\n"," \tacc test: 99.150\n","\n","Epoch 80:\n"," \tcost train: 0.026\n"," \tcost test: 0.026\n"," \tacc train: 99.162\n"," \tacc test: 99.230\n","\n","Epoch 81:\n"," \tcost train: 0.024\n"," \tcost test: 0.027\n"," \tacc train: 99.203\n"," \tacc test: 99.170\n","\n","Epoch 82:\n"," \tcost train: 0.025\n"," \tcost test: 0.026\n"," \tacc train: 99.200\n"," \tacc test: 99.080\n","\n","Epoch 83:\n"," \tcost train: 0.027\n"," \tcost test: 0.027\n"," \tacc train: 99.113\n"," \tacc test: 99.130\n","\n","Epoch 84:\n"," \tcost train: 0.024\n"," \tcost test: 0.025\n"," \tacc train: 99.205\n"," \tacc test: 99.200\n","\n","Epoch 85:\n"," \tcost train: 0.024\n"," \tcost test: 0.026\n"," \tacc train: 99.177\n"," \tacc test: 99.070\n","\n","Epoch 86:\n"," \tcost train: 0.025\n"," \tcost test: 0.027\n"," \tacc train: 99.128\n"," \tacc test: 99.140\n","\n","Epoch 87:\n"," \tcost train: 0.026\n"," \tcost test: 0.026\n"," \tacc train: 99.137\n"," \tacc test: 99.170\n","\n","Epoch 88:\n"," \tcost train: 0.024\n"," \tcost test: 0.027\n"," \tacc train: 99.192\n"," \tacc test: 99.120\n","\n","Epoch 89:\n"," \tcost train: 0.024\n"," \tcost test: 0.026\n"," \tacc train: 99.180\n"," \tacc test: 99.110\n","\n","Epoch 90:\n"," \tcost train: 0.024\n"," \tcost test: 0.025\n"," \tacc train: 99.195\n"," \tacc test: 99.140\n","\n","Epoch 91:\n"," \tcost train: 0.025\n"," \tcost test: 0.025\n"," \tacc train: 99.195\n"," \tacc test: 99.160\n","\n","Epoch 92:\n"," \tcost train: 0.026\n"," \tcost test: 0.027\n"," \tacc train: 99.142\n"," \tacc test: 99.130\n","\n","Epoch 93:\n"," \tcost train: 0.025\n"," \tcost test: 0.027\n"," \tacc train: 99.183\n"," \tacc test: 99.100\n","\n","Epoch 94:\n"," \tcost train: 0.025\n"," \tcost test: 0.025\n"," \tacc train: 99.170\n"," \tacc test: 99.170\n","\n","Epoch 95:\n"," \tcost train: 0.025\n"," \tcost test: 0.024\n"," \tacc train: 99.173\n"," \tacc test: 99.210\n","\n","Epoch 96:\n"," \tcost train: 0.022\n"," \tcost test: 0.025\n"," \tacc train: 99.300\n"," \tacc test: 99.120\n","\n","Epoch 97:\n"," \tcost train: 0.025\n"," \tcost test: 0.025\n"," \tacc train: 99.190\n"," \tacc test: 99.210\n","\n","Epoch 98:\n"," \tcost train: 0.023\n"," \tcost test: 0.028\n"," \tacc train: 99.233\n"," \tacc test: 99.040\n","\n","Epoch 99:\n"," \tcost train: 0.026\n"," \tcost test: 0.025\n"," \tacc train: 99.125\n"," \tacc test: 99.190\n","\n","Best test-acc: 0.9923\n"],"name":"stdout"}]}]}